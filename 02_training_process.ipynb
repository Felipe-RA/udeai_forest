{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING PROCESS**\n",
    "\n",
    "<BR>\n",
    "\n",
    "We will train using **three** different modeling approaches. All of their specific implementations are viewable at [src/classes](src/classes)\n",
    "\n",
    "1. MultipleRegressor\n",
    "2. ParzenWindow\n",
    "3. VGG inspired Neural Network\n",
    "\n",
    "All these models will be **compatible** with popular **Machine Learning** libraries `sklearn` or `PyTorch` through the extension of their respective `BaseModel` classes. \n",
    "\n",
    "The most resource-heavy models do not work properly within this `notebook` environments. We aim to explain the code and the training decisions here, but when its time to run the training cycles, we **strongly** suggest that you use the scripts instead.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Table of Contents\n",
    "- [**TRAINING PROCESS**](#training-process)\n",
    "\n",
    "  - [**MultipleRegressionModel.py**](#multipleregressionmodel.py)  \n",
    "    - [*Qualities of the MultipleRegressionModel.py*](#qualities-of-the-multipleregressionmodel.py)  \n",
    "\n",
    "\n",
    "  - [**ParzenWindowModel.py**](#parzenwindowmodel.py)  \n",
    "\n",
    "    - [*Qualities of the ParzenWindowModel.py*](#qualities-of-the-parzenwindowmodel.py)  \n",
    "\n",
    "  - [**VGGUdeaSpectral.py**](#vggudeaspectral.py)  \n",
    "    - [*Qualities of the VGGUdeaSpectral.py*](#qualities-of-the-vggudeaspectral.py)  \n",
    "    \n",
    "\n",
    "\n",
    "  - [**CONCLUSIONS DERIVED FROM THE RESULTS OF THE MODELS**](#conclusions-derived-from-the-results-of-the-models)  \n",
    "\n",
    "  - [FUTURE WORK AND USES FOR THE MODEL](#future-work-and-uses-for-the-model)  \n",
    "\n",
    "    - [Future Work](#future-work)  \n",
    "\n",
    "    - [**USES FOR THE MODEL**](#uses-for-the-model)  \n",
    "    \n",
    "      - [**REFERENCES**](#references)  \n",
    "\n",
    "\n",
    "<BR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## **MultipleRegressionModel.py**\n",
    "\n",
    "<br>\n",
    "\n",
    "A multiple regressor  is a model  that is used to predict the outcome of a dependent variable based on the values of multiple independent variables. This is based on the assumption that there is a linear relationship between the independent variables and the dependent variable.\n",
    "\n",
    "However, there are several reasons why a multiple regressor may not be the best option for a regression problem involving satellite images, we list here the ones that we consider most relevant:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **Non-linearity:** The relationship between the features in satellite images (such as pixel values) and the outcome variable is often complex and non-linear. Multiple regressors assume a linear relationship, which may not be sufficient to capture the patterns in satellite imagery.\n",
    "\n",
    "2. **Spatial Autocorrelation:** Pixels in satellite images are often spatially correlated with nearby pixels, violating the multiple regression assumption that observations are independent of each other.\n",
    "\n",
    "3. **Spectral Autocorrelation:** There is often a high correlation between different spectral bands in satellite images, which can lead to multicollinearity problems in multiple regression models.\n",
    "\n",
    "\n",
    "The images need to be **flattenned** to be fed into the model.\n",
    "\n",
    "<br>\n",
    "\n",
    "Nonetheless, we implement it here as an academic exercise.\n",
    "\n",
    "<br>\n",
    "\n",
    "### *Qualities of the MultipleRegressionModel.py*\n",
    "\n",
    "- **Model Definition**: Available at [src/classes/MultipleRegressionModel.py](src/classes/MultipleRegressionModel.py)\n",
    "\n",
    "- **Training Cycle**: Available at [src/generate_models/train_MultipleRegression.py](src/generate_models/train_MultipleRegression.py)\n",
    "\n",
    "- **Compatible with**: sklearn **[BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)** and **[RegressorMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html)**\n",
    "\n",
    "- **Hyperparameter Optimization SearchSpace**: \n",
    "\n",
    "    ```python\n",
    "    param_grid = {\n",
    "        'fit_intercept': [True, False],\n",
    "        'copy_X': [True, False],\n",
    "        'positive': [True, False]\n",
    "    }\n",
    "    ```\n",
    "- **Model Optimization Utility**: `GridSearchCV` with scoring to *minimize MAE* and Shuffled Kfolds with `cv=3` and a 20% val split. The `RMSE` is used as a complementary metric.\n",
    "\n",
    "    ```python\n",
    "    model = GridSearchCV(\n",
    "                        estimator=MultipleRegressionModel(),\n",
    "                        param_grid=param_grid,\n",
    "                        n_jobs=2,            # really unstable. move back to 1 if crashing. dont use -1\n",
    "                        scoring='neg_mean_absolute_error',  #  minimize MAE\n",
    "                        cv=3,                \n",
    "                        verbose=3    \n",
    "                    )\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The code provided below is a slight modification of the one available on the scripts (we changed the import structure to match the package structure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS DEFINITION\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class MultipleRegressionModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, fit_intercept=True, copy_X=True, positive=True):\n",
    "        \"\"\"\n",
    "        Hyperparameters:\n",
    "        - fit_intercept: bool, default=True\n",
    "            Whether to calculate the intercept for this model.\n",
    "        - copy_X: bool, default=True\n",
    "            If True, X will be copied; else, it may be overwritten.\n",
    "        - positive: bool, default=True\n",
    "            When set to True, forces the coefficients to be positive.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.copy_X = copy_X\n",
    "        self.positive = positive\n",
    "        \n",
    "        self.model = LinearRegression(\n",
    "            fit_intercept=self.fit_intercept, \n",
    "            copy_X=self.copy_X,\n",
    "            n_jobs=-1,  # Always use the maximum number of jobs\n",
    "            positive=self.positive\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder MultipleRegression3 created at src/trained_models/MultipleRegression3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KFold Progress: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 2/3] END copy_X=True, fit_intercept=True, positive=True;, score=-4.665 total time=   8.9s\n",
      "[CV 1/3] END copy_X=True, fit_intercept=True, positive=True;, score=-4.661 total time=   9.0s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=True, positive=True;, score=-4.606 total time=   5.6s\n",
      "[CV 1/3] END copy_X=True, fit_intercept=True, positive=False;, score=-4.661 total time=   5.8s\n",
      "[CV 2/3] END copy_X=True, fit_intercept=True, positive=False;, score=-4.665 total time=   4.4s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=True, positive=False;, score=-4.606 total time=   4.6s\n",
      "[CV 1/3] END copy_X=True, fit_intercept=False, positive=True;, score=-4.661 total time=   4.4s\n",
      "[CV 2/3] END copy_X=True, fit_intercept=False, positive=True;, score=-4.665 total time=   4.5s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=False, positive=True;, score=-4.606 total time=   4.4s\n",
      "[CV 1/3] END copy_X=True, fit_intercept=False, positive=False;, score=-4.661 total time=   4.6s\n",
      "[CV 2/3] END copy_X=True, fit_intercept=False, positive=False;, score=-4.665 total time=   4.4s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=False, positive=False;, score=-4.606 total time=   4.4s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=True, positive=True;, score=-4.661 total time=   4.4s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=True, positive=True;, score=-4.665 total time=   4.4s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=True, positive=True;, score=-4.606 total time=   4.5s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=True, positive=False;, score=-4.661 total time=   4.4s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=True, positive=False;, score=-4.665 total time=   4.5s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=True, positive=False;, score=-4.606 total time=   4.4s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=False, positive=True;, score=-4.661 total time=   4.5s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=False, positive=True;, score=-4.665 total time=   4.5s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=False, positive=True;, score=-4.606 total time=   4.5s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=False, positive=False;, score=-4.661 total time=   4.4s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=False, positive=False;, score=-4.665 total time=   4.5s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=False, positive=False;, score=-4.606 total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KFold Progress: 1it [01:06, 66.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFold 1, Validation MAE Loss: 4.670590615976926, RMSE Loss: 9.149559675514807\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END copy_X=True, fit_intercept=True, positive=True;, score=-4.756 total time=   4.4s\n",
      "[CV 2/3] END copy_X=True, fit_intercept=True, positive=True;, score=-4.672 total time=   4.5s\n",
      "[CV 1/3] END copy_X=True, fit_intercept=True, positive=False;, score=-4.756 total time=   4.5s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=True, positive=True;, score=-4.597 total time=   4.5s\n",
      "[CV 2/3] END copy_X=True, fit_intercept=True, positive=False;, score=-4.672 total time=   4.5s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=True, positive=False;, score=-4.597 total time=   4.5s\n",
      "[CV 1/3] END copy_X=True, fit_intercept=False, positive=True;, score=-4.756 total time=   4.4s\n",
      "[CV 2/3] END copy_X=True, fit_intercept=False, positive=True;, score=-4.672 total time=   4.5s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=False, positive=True;, score=-4.597 total time=   4.4s\n",
      "[CV 1/3] END copy_X=True, fit_intercept=False, positive=False;, score=-4.756 total time=   4.4s\n",
      "[CV 3/3] END copy_X=True, fit_intercept=False, positive=False;, score=-4.597 total time=   4.4s\n",
      "[CV 2/3] END copy_X=True, fit_intercept=False, positive=False;, score=-4.672 total time=   4.5s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=True, positive=True;, score=-4.756 total time=   4.4s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=True, positive=True;, score=-4.672 total time=   4.5s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=True, positive=True;, score=-4.597 total time=   4.4s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=True, positive=False;, score=-4.756 total time=   4.5s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=True, positive=False;, score=-4.672 total time=   4.4s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=True, positive=False;, score=-4.597 total time=   4.4s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=False, positive=True;, score=-4.756 total time=   4.4s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=False, positive=True;, score=-4.672 total time=   4.4s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=False, positive=True;, score=-4.597 total time=   4.4s\n",
      "[CV 1/3] END copy_X=False, fit_intercept=False, positive=False;, score=-4.756 total time=   4.5s\n",
      "[CV 2/3] END copy_X=False, fit_intercept=False, positive=False;, score=-4.672 total time=   4.5s\n",
      "[CV 3/3] END copy_X=False, fit_intercept=False, positive=False;, score=-4.597 total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KFold Progress: 2it [02:06, 63.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFold 2, Validation MAE Loss: 4.647541698041453, RMSE Loss: 9.156946782340356\n",
      "\n",
      "\n",
      "####################\n",
      "\n",
      "\n",
      "Report of training results: \n",
      "\n",
      "\n",
      "{'Best Hyperparameters': {'copy_X': True,\n",
      "                          'fit_intercept': True,\n",
      "                          'positive': True},\n",
      " 'Total Training Time (seconds)': 58.918001890182495,\n",
      " 'Validation MAE Loss (Avg)': 4.659066157009189,\n",
      " 'Validation MAE Loss (Std)': 0.011524458967736795,\n",
      " 'Validation RMSE Loss (Avg)': 9.153253228927582,\n",
      " 'Validation RMSE Loss (Std)': 0.003693553412774442}\n",
      "\n",
      "\n",
      "####################\n",
      "\n",
      "\n",
      "\n",
      "Report saved at model folder.\n",
      "Filename: src/trained_models/MultipleRegression3/MultipleRegression_report3.txt.\n",
      "\n",
      "Serialized model saved at model folder.\n",
      " Filename: src/trained_models/MultipleRegression3/MultipleRegression_model3.joblib.\n",
      "\n",
      "Hyperparameters saved at model folder.\n",
      " Filename: src/trained_models/MultipleRegression3/MultipleRegression_hyperparameters3.json.\n",
      "\n",
      "Hyperparameter optimization log saved at model folder.\n",
      " Filename: src/trained_models/MultipleRegression3/MultipleRegression_hyperparameter_optimization_log3.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## training cycle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Importing the custom model class\n",
    "from src.classes.MultipleRegressionModel import MultipleRegressionModel\n",
    "\n",
    "# Utility functions\n",
    "from src.utils import *\n",
    "\n",
    "def main():\n",
    "    # Load the data\n",
    "    X = torch.load('X_tensor.pth').numpy()\n",
    "    y = torch.load('y_tensor.pth').numpy()\n",
    "    \n",
    "    # Flatten the images\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # Initialize K-Fold cross-validation\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'fit_intercept': [True, False],\n",
    "        'copy_X': [True, False],\n",
    "        'positive': [True, False]\n",
    "    }\n",
    "\n",
    "    # Initialize the model with hyperparameter grid\n",
    "    model = GridSearchCV(\n",
    "                        estimator=MultipleRegressionModel(),\n",
    "                        param_grid=param_grid,\n",
    "                        n_jobs=2,            # really unstable. move back to 1 if crashing. dont use -1\n",
    "                        scoring='neg_mean_absolute_error',  #  minimize MAE\n",
    "                        cv=3,                \n",
    "                        verbose=3    \n",
    "                    )\n",
    "    \n",
    "    # Initialize variables for cross-validation\n",
    "    fold_mae_losses = []\n",
    "    fold_rmse_losses = []\n",
    "    \n",
    "    # Create a folder for this specific model training\n",
    "    model_type = \"MultipleRegression\"\n",
    "    model_folder = generate_unique_folder(str(model_type))\n",
    "    print(f\"Folder {model_folder[0]} created at {model_folder[1]}\")\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold, (train_index, val_index) in enumerate(tqdm(kf.split(X), desc='KFold Progress')):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        # Train the model on the training set\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate the MAE loss\n",
    "        mae_loss = mean_absolute_error(y_val, y_pred)\n",
    "        fold_mae_losses.append(mae_loss)\n",
    "\n",
    "        # Calculate the RMSE loss\n",
    "        rmse_loss = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        fold_rmse_losses.append(rmse_loss)\n",
    "      \n",
    "        # Report for the current fold\n",
    "        print(f\"\\tFold {fold + 1}, Validation MAE Loss: {mae_loss}, RMSE Loss: {rmse_loss}\")\n",
    "        \n",
    "    # Calculate the average and standard deviation of the losses over all folds\n",
    "    avg_mae_loss = np.mean(fold_mae_losses)\n",
    "    std_mae_loss = np.std(fold_mae_losses)\n",
    "    avg_rmse_loss = np.mean(fold_rmse_losses)\n",
    "    std_rmse_loss = np.std(fold_rmse_losses)\n",
    "\n",
    "    # Create the report dictionary\n",
    "    report_dict = {\n",
    "        \"Validation MAE Loss (Avg)\": avg_mae_loss,\n",
    "        \"Validation MAE Loss (Std)\": std_mae_loss,\n",
    "        \"Validation RMSE Loss (Avg)\": avg_rmse_loss,\n",
    "        \"Validation RMSE Loss (Std)\": std_rmse_loss,\n",
    "        \"Total Training Time (seconds)\": end_time - start_time,\n",
    "        \"Best Hyperparameters\": model.best_params_\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    save_and_report_model_artifacts(report_dict, model, model.best_params_, model_folder, model_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We created an utility that **serializes the model**, stores a log of the **`hyperparameter` optimization process**, creates a **report of the final results in `JSON` format** and also stores **the best hyperparameters** as a `dict` and `serializes` them to a `JSON` file under the path **`src/trained_models/[MODELNAME][MODELCOUNTER]`**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "save_and_report_model_artifacts(report_dict, model, model.best_params_, model_folder, model_type)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "The relative paths are listed at the end of the training cycle, as something  this:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "Report saved at model folder.\n",
    "Filename: src/trained_models/MultipleRegression3/MultipleRegression_report3.txt.\n",
    "\n",
    "Filename: src/trained_models/MultipleRegression3/MultipleRegression_hyperparameters3.json.\n",
    "\n",
    "Hyperparameter optimization log saved at model folder.\n",
    "Filename: src/trained_models/MultipleRegression3/MultipleRegression_hyperparameter_optimization_log3.json.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Once you run the cell above, you should get a result like this:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "####################\n",
    "\n",
    "\n",
    "Report of training results: \n",
    "\n",
    "\n",
    "{'Best Hyperparameters': {'copy_X': True,\n",
    "                          'fit_intercept': True,\n",
    "                          'positive': True},\n",
    " 'Total Training Time (seconds)': 58.918001890182495,\n",
    " 'Validation MAE Loss (Avg)': 4.659066157009189,\n",
    " 'Validation MAE Loss (Std)': 0.011524458967736795,\n",
    " 'Validation RMSE Loss (Avg)': 9.153253228927582,\n",
    " 'Validation RMSE Loss (Std)': 0.003693553412774442}\n",
    "\n",
    "####################\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "This report shows:\n",
    "\n",
    "- A Dict of the **best  hyperparameters**.\n",
    "\n",
    "- The total training time in **seconds**.\n",
    "\n",
    "- The average  **Mean Absolute Error Loss** on the validation runs as *percentage points*.\n",
    "\n",
    "- The standard deviation of the **Mean Absolute Error Loss** on the validation runs as *percentage points*.\n",
    "\n",
    "- The average **Root Mean Squared Error Loss** on the validation runs as *percentage points*.\n",
    "\n",
    "- The standard deviation of the  **Root Mean Squared Error Loss** on the validation runs as *percentage points*.\n",
    "\n",
    "<br>\n",
    "\n",
    "The Standard Deviation between each validation runs show how **stable** is the model when predicting with different data. \n",
    "\n",
    "With this specific model we got, for the **MAE Loss Metric**, a deviation of `~0.011%` between each validation run. We consider this a sign of a stable model. Meanwhile, for the **RMSE Loss Metric** we got an **Standard Deviation** of `~0.0036%`. This shows that our model is even more stable when we **punish** large errors and outliers.\n",
    "\n",
    "On the other hand, the average of the validation runs show us how does the model behave when confronting new data, and how does the results differenciate themselves from the actual target variable (`y_true`).\n",
    "\n",
    "With this specific model we got, for the **MAE Loss Metric**, an average of `~4.65%` through all the validation runs. This means that our model on average differs  $\\pm \\sim 4.65 \\% $  from the `y_true` target variable.  Meanwhile, for the **RMSE Loss Metric** we got an average of $\\sim 9.15 \\% $ from the `y_true` target variable . This shows that our model is being affected by the presence of  predictions with large errors, likely a result of the flattening of the `X` satellital images to accomodat the necessities of the `MultipleRegressor`, thus losing some of the underlying relations between the data channels and specially the spatial context of each pixel.\n",
    "\n",
    "Overall, although the results of this model are surprisingly good compared to our best performing model, taking into account that it does not accomodate the nature of the data. We expect models that **preserve the spatial relations of the images** to have a better performance, as we will see later with the `VGGUdeaSpectral` neural network.\n",
    "\n",
    "Whether `~4.65%` is a good result or not will be discussed in our next notebook.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ParzenWindowModel.py**\n",
    "\n",
    "<br>\n",
    "\n",
    "A Parzen Window model is essentially a non-parametric technique for estimating the probability density function of a random variable. When implemented using a `KNeighborsRegressor` (which was the approach choosen), it works by finding the 'k' nearest neighbors of a point and averaging their values to make a prediction. This is a type of instance-based learning where predictions are made for a new instance (data point) based on the instances of training data that are nearest in the input space.\n",
    "\n",
    "<br>\n",
    "\n",
    "For our regression problem related to Satellital Imagery, this model should not be a good fit because:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. High Dimensionality: Satellite images are high-dimensional data, where each pixel can be a feature. KNeighborsRegressor and similar instance-based methods suffer in high-dimensional spaces because the concept of \"nearest\" becomes less meaningful in high-dimensional spaces.\n",
    "\n",
    "2. Computational Intensity: Finding the nearest neighbors is computationally expensive when dealing with thousands or millions of pixels from satellite images.\n",
    "\n",
    "3. Spatial Autocorrelation: Satellite data often exhibit spatial autocorrelation, where nearby pixels are more similar to each other than to distant pixels. This spatial information is crucial and is not effectively utilized by simple distance metrics used in nearest neighbors algorithms. \n",
    "\n",
    "4. Spatial Information loss: We need to flatten the images to feed them into the ParzenWindow, thus we lose the intrinsic spatial relations between the pixels and images, alongside their interpretability.\n",
    "\n",
    "5. Non-linearity: The relationship between the spectral values in satellite images and the output variable (like vegetation coverage) is often complex and non-linear. The KNeighborsRegressor assumes that points that are near each other in feature space have similar output values, which may not always be true for the complex patterns present in satellite images.\n",
    "\n",
    "6. Storage Requirement: Instance-based methods require storing the entire dataset in memory to make predictions, this was not a problem for us since we had enough `RAM` to load the whole dataset. But if we had trained with bigger images or more channels, we will quickly ran out of memory.\n",
    "\n",
    "<br>\n",
    "\n",
    "For these reasons, we do not expect good results for this model. Nonetheless, we implement it here as an academic exercise.\n",
    "\n",
    "<br>\n",
    "\n",
    "### *Qualities of the ParzenWindowModel.py*\n",
    "\n",
    "- **Model Definition**: Available at [src/classes/ParzenWindowModel.py](src/classes/ParzenWindowModel.py)\n",
    "\n",
    "- **Training Cycle**: Available at [src/generate_models/train_ParzenWindow.py](src/generate_models/train_ParzenWindow.py)\n",
    "\n",
    "- **Compatible with**: sklearn **[BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)** and **[RegressorMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html)**\n",
    "\n",
    "- **Hyperparameter Optimization SearchSpace**: \n",
    "\n",
    "    ```python\n",
    "    param_dist = {\n",
    "        'n_neighbors': [3, 5, 7, 10],\n",
    "        'weights': ['uniform', 'distance', 'gaussian'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(15, 46),\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "    ```\n",
    "- **Model Optimization Utility**: `RandomizedSearchCV` with scoring to *minimize MAE* and Shuffled Kfolds with `cv=3` and a 20% val split. The `RMSE` is used as a complementary metric.\n",
    "\n",
    "    ```python\n",
    "    model = RandomizedSearchCV(\n",
    "                                ParzenWindowRegressor(),\n",
    "                                param_distributions=param_dist,\n",
    "                                n_jobs=2,            # really unstable. move back to 1 if crashing\n",
    "                                n_iter=30,\n",
    "                                cv=3,\n",
    "                                verbose=3,\n",
    "                                random_state=None,\n",
    "                                scoring='neg_mean_absolute_error',  #  minimize MAE\n",
    "                            )\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The code provided below is a slight modification of the one available on the scripts (we changed the import structure to match the package structure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  MODEL DEFINITION\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class ParzenWindowRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_neighbors=5, weights='uniform', algorithm='auto',\n",
    "                 leaf_size=30, p=2, metric='minkowski', metric_params=None):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.algorithm = algorithm\n",
    "        self.leaf_size = leaf_size\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        self.metric_params = metric_params\n",
    "        self.model = KNeighborsRegressor(\n",
    "            n_neighbors=self.n_neighbors, \n",
    "            weights=self.weights, \n",
    "            algorithm=self.algorithm, \n",
    "            leaf_size=self.leaf_size, \n",
    "            p=self.p, \n",
    "            metric=self.metric,\n",
    "            metric_params=self.metric_params\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**\n",
    "\n",
    "The code below **crashes** on our `Jupyter` environments, likely due to the high requirements of memory and computing power. We will show the results obtained from the [script]([src/generate_models/train_ParzenWindow.py](src/generate_models/train_ParzenWindow.py)), but we will not execute the next cell.\n",
    "\n",
    "The next cell will be left as an illustrative example.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING CYCLE\n",
    "\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils import generate_unique_folder, save_and_report_model_artifacts\n",
    "\n",
    "# Import custom class\n",
    "from src.classes.ParzenWindowModel import ParzenWindowRegressor\n",
    "\n",
    "def main():\n",
    "    # Load the data\n",
    "    X = torch.load('X_tensor.pth').numpy()\n",
    "    y = torch.load('y_tensor.pth').numpy()\n",
    "    \n",
    "    # Flatten the images\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # Initialize K-Fold cross-validation # this is legacy since we implemented hyperparam_search with CV\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=None) \n",
    "    \n",
    "    # Hyperparameter distr dict to search\n",
    "    param_dist = {\n",
    "        'n_neighbors': [3, 5, 7, 10],\n",
    "        'weights': ['uniform', 'distance', 'gaussian'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski', 'chebyshev'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(15, 46),\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "    \n",
    "    # Initialize the model with hyperparameter grid\n",
    "    model = RandomizedSearchCV(\n",
    "                                ParzenWindowRegressor(),\n",
    "                                param_distributions=param_dist,\n",
    "                                n_jobs=2,            # really unstable. move back to 1 if crashing\n",
    "                                n_iter=30,\n",
    "                                cv=3,\n",
    "                                verbose=3,\n",
    "                                random_state=None,\n",
    "                                scoring='neg_mean_absolute_error',  #  minimize MAE\n",
    "                            )\n",
    "\n",
    "    \n",
    "    # Create a folder for this specific model training\n",
    "    model_type = \"ParzenWindow\"\n",
    "    model_folder = generate_unique_folder(model_type)\n",
    "    print(f\"Folder {model_folder[0]} created at {model_folder[1]}\")\n",
    "    \n",
    "    # Initialize variables for cross-validation\n",
    "    fold_mae_losses = []\n",
    "    fold_rmse_losses = []\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold, (train_index, val_index) in enumerate(tqdm(kf.split(X), desc='KFold Progress')):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        # Train the model on the training set\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate the MAE loss\n",
    "        mae_loss = mean_absolute_error(y_val, y_pred)\n",
    "        fold_mae_losses.append(mae_loss)\n",
    "        \n",
    "        # Calculate the RMSE loss\n",
    "        rmse_loss = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        fold_rmse_losses.append(rmse_loss)\n",
    "        \n",
    "        # Report for the current fold\n",
    "        print(f\"Fold {fold + 1}, Validation MAE Loss: {mae_loss}, RMSE Loss: {rmse_loss}\")\n",
    "    \n",
    "    # Calculate the average and standard deviation of the losses over all folds\n",
    "    avg_mae_loss = np.mean(fold_mae_losses)\n",
    "    std_mae_loss = np.std(fold_mae_losses)\n",
    "    avg_rmse_loss = np.mean(fold_rmse_losses)\n",
    "    std_rmse_loss = np.std(fold_rmse_losses)\n",
    "    \n",
    "    # Create the report dictionary\n",
    "    report_dict = {\n",
    "        \"Validation MAE Loss (Avg)\": avg_mae_loss,\n",
    "        \"Validation MAE Loss (Std)\": std_mae_loss,\n",
    "        \"Validation RMSE Loss (Avg)\": avg_rmse_loss,\n",
    "        \"Validation RMSE Loss (Std)\": std_rmse_loss,\n",
    "        \"Total Training Time (seconds)\": end_time - start_time,\n",
    "        \"Best Hyperparameters\": model.best_params_,\n",
    "    }\n",
    "    \n",
    "    save_and_report_model_artifacts(report_dict, model, model.best_params_, model_folder, model_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Like in our `MultipleRegressorModel.py`, we generate a series of artifacts. We store a log of the **`hyperparameter` optimization process**, create a **report of the final results in `JSON` format** and also store **the best hyperparameters** as a `dict` and `serializes` them to a `JSON` file under the path **`src/trained_models/[MODELNAME][MODELCOUNTER]`**.\n",
    "\n",
    "<br>\n",
    "\n",
    "At our `trained_models` directory, we fetch the `ParzenWindow0` folder and show next the results of the training.\n",
    "\n",
    "<br>\n",
    "\n",
    "From `src/trained_models/ParzenWindow0/ParzenWindow_report0.txt` we get:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"Validation MAE Loss (Avg)\": 10.121114730834961,\n",
    "    \"Validation MAE Loss (Std)\": 0.745750904083252,\n",
    "    \"Validation RMSE Loss (Avg)\": 17.552146911621094,\n",
    "    \"Validation RMSE Loss (Std)\": 1.4414606094360352,\n",
    "    \"Total Training Time (seconds)\": 2867.5542833805084,\n",
    "    \"Best Hyperparameters\": {\n",
    "        \"algorithm\": \"kd_tree\",\n",
    "        \"leaf_size\": 18,\n",
    "        \"metric\": \"minkowski\",\n",
    "        \"n_neighbors\": 10,\n",
    "        \"p\": 2,\n",
    "        \"weights\": \"gaussian\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "We can observe that:\n",
    "\n",
    "- The stability of the `ParzenWindowModel` is significantly worse than our `MultipleRegressor`, approximately **`~390`** times less stable between validation runs for the **RMSE Loss Metric** and **`64.71`** times less stable between validation runs for the **MAE Loss Metric**. \n",
    "\n",
    "- The results were more than twice worse for this model for the **MAE Loss Metric** compared to the `MultipleRegressorModel.py`\n",
    "\n",
    "- The results were `0.87` times worse for this model for the **RMSE Loss Metric** compared to the `MultipleRegressorModel.py`\n",
    "\n",
    "- The chosen distance metric was `minkowsky` distance in `p` mode 2. Which simplifies to euclidean distance.\n",
    "\n",
    "<br>\n",
    "\n",
    "Overall, and expectably, this model shows worse performance, stability and a complexity of over 58 times compared with the MultipleRegressor. We can conclude from this results that this modelling strategy does not accomodate well enough to the particularities of our satellital imagery data as discussed on the [ParzenWindowModel Definition](#parzenwindowmodelpy)\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## **VGGUdeaSpectral.py**\n",
    "\n",
    "<br>\n",
    "\n",
    "`VGGUdeaSpectral` is a custom convolutional neural network implemented by us from the `Pytorch` dependencies to extend and harness utility from its functionalities.\n",
    "\n",
    "We defined its custom architecture based off the *Visual Geometry Group (VGG)* proposed on the paper **Very Deep Convolutional Neural Networks for Large-Scale Image Recognition** by Simonyan and Zisserman in 2014. \n",
    "\n",
    "Our specific architecture uses the signature *3x3 convolutional layers* with **sequential doubling of the number of filters after each max pooling layer**\n",
    "\n",
    "The unique additions of our architecture are:\n",
    "\n",
    "- **Band customizability**: The traditional `VGGs` only accept three channels (for RGB). We extend over this functionality to accept any number of bands that might be fed to the network .\n",
    "\n",
    "- **Spectral Attention**: The SpectralAttention mechanism is a type of attention mechanism that focuses on the spectral characteristics of the input data this means the mechanism pays more attention to certain  bands of the images that are more informative for the task at hand (vegetation coverage).\n",
    "\n",
    "- **Built-in batch Normalization and Dropout**: The traditional `VGGs` do not apply these stabilization and regularization techniques by default. We include them within the architecture to handle the common  noisiness of satellital data.\n",
    "\n",
    "- **Complexity simplification by reducing convolutiona layers and stages**: Thanks to the `SpectralAttention` mechanism, our objective is to be able to tradeoff the inclusion of the `SpectralAttention` (which we expect it will lead the network to learn more efficient data patterns)  thus allowing us to reduce the complexity of the whole network. This approach allows us to keep overfitting in check while also reducing computing costs.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### *Qualities of the VGGUdeaSpectral.py*\n",
    "\n",
    "- **Model Definition**: Available at [src/classes/VGGUdeaSpectral.py](src/classes/VGGUdeaSpectral.py)\n",
    "\n",
    "- **Training Cycle**: Available at [src/generate_models/train_VGGUdeaSpectral.py](src/generate_models/train_VGGUdeaSpectral.py)\n",
    "\n",
    "- **Compatible with**: PyTorch **[NeuralNetwork base module (nn)](https://pytorch.org/docs/stable/nn.html)**\n",
    "\n",
    "- **GPU Compatibility**: YES (if your hardware is compatible and you installed the dependencies specified at the [README.MD](README.MD))\n",
    "\n",
    "- **Hyperparameter Optimization SearchSpace**: \n",
    "\n",
    "    ```python\n",
    "    param_dist = {\n",
    "        'module__num_filters1': [32, 64, 128],\n",
    "        'module__num_filters2': [64, 128, 256],\n",
    "        'module__num_filters3': [128, 256, 512],\n",
    "        'module__activation_type': ['relu', 'sigmoid', 'tanh'],\n",
    "        'module__dropout_rate': uniform(0.2, 0.5),\n",
    "        'module__fc1_out_features': [512, 1024, 2048],\n",
    "        'module__fc2_out_features': [256, 512, 1024],\n",
    "        'lr': [0.01, 0.001, 0.0001],\n",
    "        'max_epochs': [5, 10, 20]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "\n",
    "- **Model Optimization Utility**: We used a compatibility layer between `PyTorch` and `sklearn` called **`skorch`**.\n",
    "    \n",
    "    This allows us to use `RandomizedSearchCV` with scoring to *minimize MAE* and Shuffled Kfolds with `cv=3` and a 20% val split. The `RMSE` is used as a complementary metric.\n",
    "\n",
    "    <br>\n",
    "\n",
    "    ```python\n",
    "    net = NeuralNetRegressor(\n",
    "        VGGUdeaSpectral,\n",
    "        module__num_bands=3,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__shuffle=False,\n",
    "        max_epochs=10,\n",
    "        lr=0.01,\n",
    "        batch_size=32,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        train_split=ValidSplit(cv=0.2, stratified=False),\n",
    "        callbacks=[EpochScoring(rmse_score, name='valid_rmse', lower_is_better=False)]\n",
    "    )\n",
    "\n",
    "\n",
    "    model = RandomizedSearchCV(\n",
    "                            net,\n",
    "                            param_distributions=param_dist,\n",
    "                            n_iter=10,\n",
    "                            cv=3,\n",
    "                            verbose=3,\n",
    "                            random_state=42,\n",
    "                            scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    ```\n",
    "    <br>\n",
    "\n",
    "    Due to the increased computing cost of training this model, the `n_iter` was defined to a low number of *10 iterations* (which combined with the cross validation process still amounts to 30 different fit steps)\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The code provided below is a slight modification of the one available on the scripts (we changed the import structure to match the package structure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL DEFINITION\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.classes.SpectralAttention import SpectralAttention\n",
    "\n",
    "class VGGUdeaSpectral(nn.Module):\n",
    "    def __init__(self, num_bands, \n",
    "                 num_filters1=64, \n",
    "                 num_filters2=128, \n",
    "                 num_filters3=256, \n",
    "                 activation_type='relu', \n",
    "                 dropout_rate=0.5,\n",
    "                 fc1_out_features=1024,\n",
    "                 fc2_out_features=512,\n",
    "                 number_out_features=1):\n",
    "        \n",
    "        super(VGGUdeaSpectral, self).__init__()\n",
    "        \n",
    "        # Activation function selection\n",
    "        if activation_type == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation_type == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation_type == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation_type\")\n",
    "        \n",
    "        self.conv_stage1 = nn.Sequential(\n",
    "            nn.Conv2d(num_bands, num_filters1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters1),\n",
    "            SpectralAttention(num_filters1),\n",
    "            self.activation,\n",
    "            nn.Conv2d(num_filters1, num_filters1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters1),\n",
    "            SpectralAttention(num_filters1),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv_stage2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters1, num_filters2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters2),\n",
    "            SpectralAttention(num_filters2),\n",
    "            self.activation,\n",
    "            nn.Conv2d(num_filters2, num_filters2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters2),\n",
    "            SpectralAttention(num_filters2),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv_stage3 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters2, num_filters3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters3),\n",
    "            SpectralAttention(num_filters3),\n",
    "            self.activation,\n",
    "            nn.Conv2d(num_filters3, num_filters3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters3),\n",
    "            SpectralAttention(num_filters3),\n",
    "            self.activation,\n",
    "            nn.Conv2d(num_filters3, num_filters3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_filters3),\n",
    "            SpectralAttention(num_filters3),\n",
    "            self.activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((12, 12))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(num_filters3 * 12 * 12, fc1_out_features),\n",
    "            nn.BatchNorm1d(fc1_out_features),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(fc1_out_features, fc2_out_features),\n",
    "            nn.BatchNorm1d(fc2_out_features),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(fc2_out_features, number_out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stage1(x)\n",
    "        x = self.conv_stage2(x)\n",
    "        x = self.conv_stage3(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**\n",
    "\n",
    "The code below **crashes** on our `Jupyter` environments, likely due to the high requirements of memory and computing power. We will show the results obtained from the [script](src/generate_models/train_VGGUdeaSpectral.py), but we will not execute the next cell.\n",
    "\n",
    "The next cell will be left as an illustrative example.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING CYCLE\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.dataset import ValidSplit\n",
    "from skorch.callbacks import EpochScoring\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Custom utility functions\n",
    "from src.utils import generate_unique_folder, save_and_report_model_artifacts\n",
    "\n",
    "# Import the custom VGGUdeaSpectral class\n",
    "from src.classes.VGGUdeaSpectral import VGGUdeaSpectral\n",
    "\n",
    "def rmse_score(net, X, y):\n",
    "    y_pred = net.predict(X)\n",
    "    rmse = (mean_squared_error(y_true=y, y_pred=y_pred)) ** 0.5\n",
    "    return -rmse  # Skorch tries to maximize the score, so negate the RMSE\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    X = torch.load('X_tensor.pth')\n",
    "    y = torch.load('y_tensor.pth').view(-1,1)\n",
    "\n",
    "    # Convert X to float32\n",
    "    X = X.to(dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Initialize NeuralNetRegressor\n",
    "    net = NeuralNetRegressor(\n",
    "        VGGUdeaSpectral,\n",
    "        module__num_bands=3,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__shuffle=False,\n",
    "        max_epochs=10,\n",
    "        lr=0.01,\n",
    "        batch_size=32,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        train_split=ValidSplit(cv=0.2, stratified=False),\n",
    "        callbacks=[EpochScoring(rmse_score, name='valid_rmse', lower_is_better=False)]\n",
    "    )\n",
    "\n",
    "    param_dist = {\n",
    "        'module__num_filters1': [32, 64, 128],\n",
    "        'module__num_filters2': [64, 128, 256],\n",
    "        'module__num_filters3': [128, 256, 512],\n",
    "        'module__activation_type': ['relu', 'sigmoid', 'tanh'],\n",
    "        'module__dropout_rate': uniform(0.2, 0.5),\n",
    "        'module__fc1_out_features': [512, 1024, 2048],\n",
    "        'module__fc2_out_features': [256, 512, 1024],\n",
    "        'lr': [0.01, 0.001, 0.0001],\n",
    "        'max_epochs': [5, 10, 20]\n",
    "    }\n",
    "\n",
    "    model = RandomizedSearchCV(net, param_distributions=param_dist, n_iter=10, cv=3, verbose=3, random_state=42, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    model.fit(X, y)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Create the report dictionary\n",
    "    report_dict = {\n",
    "        \"Validation MAE Loss (Best)\": -model.best_score_,\n",
    "        \"Validation RMSE (Best)\": -rmse_score(model, X, y),\n",
    "        \"Total Training Time (seconds)\": end_time - start_time,\n",
    "        \"Best Hyperparameters\": model.best_params_,\n",
    "    }\n",
    "\n",
    "    # Create a folder for this specific model training\n",
    "    model_type = \"VGGUdeaSpectral\"\n",
    "    model_folder = generate_unique_folder(model_type)\n",
    "\n",
    "    # Save model and report\n",
    "    save_and_report_model_artifacts(report_dict, model, model.best_params_, model_folder, model_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Like in our `MultipleRegressorModel.py` and `ParzenWindow.py`, we generate a series of artifacts. We store a log of the **`hyperparameter` optimization process**, create a **report of the final results in `JSON` format** and also store **the best hyperparameters** as a `dict` and `serializes` them to a `JSON` file under the path **`src/trained_models/[MODELNAME][MODELCOUNTER]`**.\n",
    "\n",
    "<br>\n",
    "\n",
    "At our `trained_models` directory, we fetch the `VGGUdeaSpectral0` folder and show next the results of the training.\n",
    "\n",
    "<br>\n",
    "\n",
    "From `src/trained_models/VGGUdeaSpectral0/VGGUdeaSpectral_report0.txt` we get:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"Validation MAE Loss\": 3.799692153930664,\n",
    "    \"Validation RMSE Loss\": 6.044155090705238,\n",
    "    \"Total Training Time (seconds)\": 28129.791773557663,\n",
    "    \"Best Hyperparameters\": {\n",
    "        \"lr\": 0.0001,\n",
    "        \"max_epochs\": 20,\n",
    "        \"module__activation_type\": \"tanh\",\n",
    "        \"module__dropout_rate\": 0.6330880728874676,\n",
    "        \"module__fc1_out_features\": 2048,\n",
    "        \"module__fc2_out_features\": 512,\n",
    "        \"module__num_filters1\": 32,\n",
    "        \"module__num_filters2\": 128,\n",
    "        \"module__num_filters3\": 256\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## **CONCLUSIONS DERIVED FROM THE RESULTS OF THE MODELS**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "From these results, we can observe that:\n",
    "\n",
    "- Even with the added raw power of the `GPU` training, this model takes  `~ x488.85`  more time to train than the `MultipleRegressionModel` and `~ x9.8` more time to train than the `ParzenWindowModel`. The computational cost is evident.\n",
    "\n",
    "- On average, each fit on the  `VGGUdeaSpectral` convolutional neural network takes `~15 minutes` with our equipment. (64 DDR4 RAM 2999MHz / Ryzen 7 5800x / RX7900 XT on Ubuntu 21.03 LTS with the dependencies specified on the [requirements.txt](requirements.txt))\n",
    "\n",
    "- Our MAE Loss improved from  `~4.65` on our `MultipleRegressionModel` to `~3.8` on our `VGGUdeaSpectral` neural network.\n",
    "\n",
    "- Our RMSE Loss improved from `~9.15` on our `MultipleRegressionModel` to `~6.04` on our `VGGUdeSpectral` neural network. This means that our neural network is significantly more robust when dealing with predictions related to outliers and that the errors are more close to the target!\n",
    "\n",
    "- Our search on the *parameter space* was extremely shallow. We only tried 10 different combinations of parameters, if we were to do a `grid_search` the total number of combinations would be `19683`. \n",
    "\n",
    "\n",
    "So, we can summarize our results with:\n",
    "\n",
    "1. The best performance comes from the `VGGUdeaSpectral` model.\n",
    "\n",
    "2. The most robust (the one most resistant to outliers and most consistent with predictions near-target) is the `VGGUdeaSpectral` model.\n",
    "\n",
    "3. **Our Specific** implementation of the  `ParzenWindow` is more computational complex, less stable and with the worse performance when compared with the `VGGUdeaSpectral` model and the `MultipleRegressor` model.\n",
    "\n",
    "4. Our `VGGUdeaSpectral` model takes `~ x488.85`  more time expensive to train than our `MultipleRegressor` model. But offers an improvement of `~18.28%` on the MAE Loss metric and a `~33.99%` improvement on the RMSE Loss Metric.\n",
    "\n",
    "5. The **parametric space** of the `VGGUdeaSpectral` model was only shallowly explored.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## FUTURE WORK AND USES FOR THE MODEL\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Future Work\n",
    "\n",
    "<br>\n",
    "\n",
    "To further improve on the model, we recommend the following work:\n",
    "\n",
    "1. Expand the hyperparameter search optimization. We made an estimation based on our avg training time for each fit performed.\n",
    "\n",
    "This table shows an estimation of the minutes, hours and days that it would take those `n_iterations` :\n",
    "\n",
    "| n_iterations | Total Training Time (Minutes) | Total Training Time (Hours) | Total Training Time (Days) |\n",
    "|--------------|------------------------------|-----------------------------|----------------------------|\n",
    "| 1            | 45.0                         | 0.75                        | 0.03125                    |\n",
    "| 2            | 90.0                         | 1.5                         | 0.0625                     |\n",
    "| 3            | 135.0                        | 2.25                        | 0.09375                    |\n",
    "| 4            | 180.0                        | 3.0                         | 0.125                      |\n",
    "| 5            | 225.0                        | 3.75                        | 0.15625                    |\n",
    "| 6            | 270.0                        | 4.5                         | 0.1875                     |\n",
    "| 7            | 315.0                        | 5.25                        | 0.21875                    |\n",
    "| 8            | 360.0                        | 6.0                         | 0.25                       |\n",
    "| 9            | 405.0                        | 6.75                        | 0.28125                    |\n",
    "| 10           | 450.0                        | 7.5                         | 0.3125                     |\n",
    "| 20           | 900.0                        | 15.0                        | 0.625                      |\n",
    "| 30           | 1350.0                       | 22.5                        | 0.9375                     |\n",
    "| 40           | 1800.0                       | 30.0                        | 1.25                       |\n",
    "| 50           | 2250.0                       | 37.5                        | 1.5625                     |\n",
    "| 100          | 4500.0                       | 75.0                        | 3.125                      |\n",
    "| 200          | 9000.0                       | 150.0                       | 6.25                       |\n",
    "| 300          | 13500.0                      | 225.0                       | 9.375                      |\n",
    "| 400          | 18000.0                      | 300.0                       | 12.5                       |\n",
    "| 500          | 22500.0                      | 375.0                       | 15.625                     |\n",
    "| 1000         | 45000.0                      | 750.0                       | 31.25                      |\n",
    "| 2000         | 90000.0                      | 1500.0                      | 62.5                       |\n",
    "| 3000         | 135000.0                     | 2250.0                      | 93.75                      |\n",
    "| 4000         | 180000.0                     | 3000.0                      | 125                        |\n",
    "| 5000         | 225000.0                     | 3750.0                      | 156.25                     |\n",
    "| 6000         | 270000.0                     | 4500.0                      | 187.5                      |\n",
    "| 7000         | 315000.0                     | 5250.0                      | 218.75                     |\n",
    "| 8000         | 360000.0                     | 6000.0                      | 250                        |\n",
    "| 9000         | 405000.0                     | 6750.0                      | 281.25                     |\n",
    "| 10000        | 450000.0                     | 7500.0                      | 312.5                      |\n",
    "\n",
    "<br>\n",
    "\n",
    "Perhaps *100 different combinations* (3 days) would be a good start.\n",
    "\n",
    "\n",
    "2. Try out more bands! Our `VGGUdeaSpectral` accepts more than 3 bands. This would require an analysis process to select the best candidates to add.\n",
    "\n",
    "3. To test the **generalization power of the model even further**, we recommend to test it  with even more new data!. A good start would be to test a different year or images from a different `Instrument`; if the results are good a natural step would be to test on a different geographical **Region of Interest** (`ROI`)\n",
    "\n",
    "4. Explore the usage of  more advanced techniques like Bayesian Optimization to optimize the hyperparameters.\n",
    "\n",
    "5. Explore the performance of the model when receiving images of higher dimensions. The training was done with *100x100* images, how would the model behave with images of higher resolution?\n",
    "\n",
    "6. Explore the impact on computational resources when training the model with higher dimensional images. Will it make the `VGGUdeaSpectral` not feasible to be trained anymore?\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### **USES FOR THE MODEL**\n",
    "\n",
    "<br>\n",
    "\n",
    "We recognize that our knowledge on the subjects of *ecology* and *geology* are limited, and for us to define **performance thresholds** we will require the  advice of experts on the subjects.\n",
    "\n",
    "However, based on a review of current literature we find feasible to implement this model (with proper validation with field experts) for applications such as:\n",
    "\n",
    "1. **Rapid assessment of endangered ecosystems.** It has been found that the usage of *Satellital Imagery* is feasible to monitor the changes in **Mangrobe Forests** [1]. With the help of field experts, perhaps the same approach could be applied to the Antioquian Andean Forests and tropical rainforests.\n",
    "\n",
    "2. **Land use analysis**. Loss of vegetation is related to land development or illegal exploitations [2]. We find worthy of analysis if by systematically analyzing areas with non-progressive loss of vegetation we could detect illegal logging or mining operations in Antioquia.\n",
    "\n",
    "3. **Natural Disaster Prediction**. The loss of vegetation alongside geographical landmarks or pluvial sources has been related to landslides and floods [3]. Remote sensing techniques based on Satellital Imagery are already being used in Asia[4], perhaps our model would be a good tool to systematically analyse specific features alongside a model that is capable of identifying frontiers and borders of rivers and other ground/rainwater resources.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### **REFERENCES**\n",
    "\n",
    "\n",
    "[1] C. -H. Shih, P. -C. Chen, Y. -J. Shih, T. -J. Chu and Y. -M. Lu, \"Feasibility for Rapid Assessment of Mangroves by the Application of Satellite Imagery Technique,\" 2019 IEEE International Conference on Architecture, Construction, Environment and Hydraulics (ICACEH), Xiamen, China, 2019, pp. 44-46, doi: 10.1109/ICACEH48424.2019.9042101.\n",
    "\n",
    "[2] Yumin Tan, Bingxin Bai and M. S. Mohammad, \"Time series remote sensing based dynamic monitoring of land use and land cover change,\" 2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA), Guangzhou, China, 2016, pp. 202-206, doi: 10.1109/EORSA.2016.7552797.\n",
    "\n",
    "[3] J. Sun et al., \"Regional-Scale Monitoring of Rice Flood Disaster Based on Multi-Temporal Remote Sensing Images,\" 2018 7th International Conference on Agro-geoinformatics (Agro-geoinformatics), Hangzhou, China, 2018, pp. 1-4, doi: 10.1109/Agro-Geoinformatics.2018.8476147.\n",
    "\n",
    "[4] M. Kalidhas and R. Sivakumar, \"Image processing and Supervised Classification of LANDSAT data for Flood Impact Assessment on Land Use and Land Cover,\" 2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS), Tashkent, Uzbekistan, 2022, pp. 437-440, doi: 10.1109/ICTACS56270.2022.9988164.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
