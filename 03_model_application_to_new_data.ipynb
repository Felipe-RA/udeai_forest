{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **APPLYING OUR MODEL TO A NEW DATASET DEFINITION**\n",
    "\n",
    "---\n",
    "\n",
    "We want to explore the behaviour of our model when facing a new dataset definition.\n",
    "\n",
    "THis time, we will use a new year, `2019`, to test the predictive power of our models.\n",
    "\n",
    "The aim of this notebook is to **show if our models retain their predictive capacity when tested on a different year** while also **showcasing the use of our serialized models**.\n",
    "\n",
    "\n",
    "\n",
    "This notebook, unlike the others, will be short.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VERIFYING DATASET INTEGRITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Exploring folder: /home/felipera/projects/udeai_forest/data/sentinel2rgbmedian2020.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/sentinel2rgbmedian2020.py: 100%|██████████| 64106/64106 [01:22<00:00, 773.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "\n",
      "Unique Configurations:\n",
      "\n",
      "Configuration #1:\n",
      "  Width: 100, Height: 100\n",
      "  Data Types: ('uint8', 'uint8', 'uint8')\n",
      "  CRS: EPSG:4326\n",
      "  Number of Bands: 3\n",
      "  Count of Images with this Configuration: 64106\n",
      "=======================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Exploring folder: /home/felipera/projects/udeai_forest/data/treecover2020.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data/treecover2020.py: 100%|██████████| 64106/64106 [01:21<00:00, 783.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "\n",
      "Unique Configurations:\n",
      "\n",
      "Configuration #1:\n",
      "  Width: 100, Height: 100\n",
      "  Data Types: ('uint8', 'uint8', 'uint8')\n",
      "  CRS: EPSG:4326\n",
      "  Number of Bands: 3\n",
      "  Count of Images with this Configuration: 64106\n",
      "=======================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def list_tif_files(folder_path):\n",
    "    return [f for f in os.listdir(folder_path) if f.endswith('.tif')]\n",
    "\n",
    "def explore_folder(folder_path):\n",
    "    config_count = defaultdict(int)\n",
    "    \n",
    "    tif_files = list_tif_files(folder_path)\n",
    "    for tif_file in tqdm(tif_files, desc=f\"Processing {folder_path}\"): \n",
    "        file_path = os.path.join(folder_path, tif_file)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            config = (\n",
    "                src.width,\n",
    "                src.height,\n",
    "                tuple(src.dtypes),\n",
    "                str(src.crs),\n",
    "                src.count\n",
    "            )\n",
    "            config_count[config] += 1\n",
    "    \n",
    "    return config_count\n",
    "\n",
    "def explore_tif_folders(folder_paths):\n",
    "    for folder_path in folder_paths:\n",
    "        absolute_folder_path = os.path.abspath(folder_path)\n",
    "        print(f\"\\n\\n\\nExploring folder: {absolute_folder_path}\")\n",
    "\n",
    "        config_count = explore_folder(folder_path)\n",
    "        print(\"=======================================\")\n",
    "        print(\"\\nUnique Configurations:\")\n",
    "        for i, (config, count) in enumerate(config_count.items(), 1):\n",
    "            width, height, dtypes, crs, bands = config\n",
    "            print(f\"\\nConfiguration #{i}:\")\n",
    "            print(f\"  Width: {width}, Height: {height}\")\n",
    "            print(f\"  Data Types: {dtypes}\")\n",
    "            print(f\"  CRS: {crs}\")\n",
    "            print(f\"  Number of Bands: {bands}\")\n",
    "            print(f\"  Count of Images with this Configuration: {count}\")\n",
    "        print(\"=======================================\\n\")\n",
    "\n",
    "\n",
    "# Initial folder paths, relative to the current script location\n",
    "\n",
    "tif_dataset_paths = [\n",
    "    'data/sentinel2rgbmedian2020.py',\n",
    "    'data/treecover2020.py'\n",
    "]\n",
    "explore_tif_folders(tif_dataset_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the configurations match, we know that the data was downloaded without errors.\n",
    "\n",
    "<br>\n",
    "\n",
    "<BR>\n",
    "\n",
    "---\n",
    "\n",
    "## **SERIALIZING DATASETS TO PREPARE FOR PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64106/64106 [03:21<00:00, 318.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation and serialization complete.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to read and process a single .tif file for 'y'\n",
    "def read_and_process_tif_file_for_y(filepath):\n",
    "    with rasterio.open(filepath) as src:\n",
    "        # Read the bands 1 and 3, skipping the empty band 2\n",
    "        band1, band3 = src.read(1), src.read(3)\n",
    "        \n",
    "        # Apply the transformation to scale the values between 0 and 100\n",
    "        band1 = (band1 / 255.0) * 100\n",
    "        band3 = (band3 / 255.0) * 100\n",
    "        \n",
    "        # Calculate Percent_Vegetation_Coverage and clip it to be between 0 and 100\n",
    "        percent_vegetation_coverage = np.clip(band1 + band3, 0, 100)\n",
    "        \n",
    "        # Calculate a single Percent_Vegetation_Coverage value for the entire image (e.g., mean)\n",
    "        single_value = np.mean(percent_vegetation_coverage)\n",
    "        \n",
    "        return single_value, src.meta\n",
    "\n",
    "\n",
    "# Function to read a single .tif file and return as numpy array\n",
    "def read_tif_file(filepath):\n",
    "    with rasterio.open(filepath) as src:\n",
    "        return np.array(src.read()), src.meta\n",
    "\n",
    "# Function to save metadata to a JSON file\n",
    "def save_metadata(metadata_dict, save_path):\n",
    "    for meta in metadata_dict:\n",
    "        if 'sentinel_meta' in meta and 'crs' in meta['sentinel_meta']:\n",
    "            meta['sentinel_meta']['crs'] = str(meta['sentinel_meta']['crs'])\n",
    "        if 'treecover_meta' in meta and 'crs' in meta['treecover_meta']:\n",
    "            meta['treecover_meta']['crs'] = str(meta['treecover_meta']['crs'])\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(metadata_dict, f)\n",
    "\n",
    "\n",
    "# Initialize empty lists to store images and metadata\n",
    "X_images = []\n",
    "y_images = []\n",
    "metadata_list = []\n",
    "\n",
    "\n",
    "### ---------------------------- ###\n",
    "\n",
    "# Directory paths\n",
    "sentinel_dir = 'data/sentinel2rgbmedian2020.py'  # Replace with your actual directory\n",
    "treecover_dir = 'data/treecover2020.py'  # Replace with your actual directory\n",
    "\n",
    "X_test_tensor_name = 'X_test_tensor_visuals.pth'\n",
    "y_test_tensor_name = 'y_test_tensor_visuals.pth'\n",
    "\n",
    "### ---------------------------- ###\n",
    "\n",
    "\n",
    "# File names are assumed to be the same in both directories\n",
    "filenames = os.listdir(sentinel_dir)\n",
    "\n",
    "# Loop through each file and read the image and metadata\n",
    "for filename in tqdm(filenames):\n",
    "    # Read SENTINEL-2 image\n",
    "    sentinel_path = os.path.join(sentinel_dir, filename)\n",
    "    sentinel_img, sentinel_meta = read_tif_file(sentinel_path)\n",
    "    \n",
    "    # Read and process Tree Cover image\n",
    "    treecover_path = os.path.join(treecover_dir, filename)\n",
    "    percent_vegetation_coverage, treecover_meta = read_and_process_tif_file_for_y(treecover_path)\n",
    "    \n",
    "    # Append to lists\n",
    "    X_images.append(sentinel_img)\n",
    "    y_images.append(percent_vegetation_coverage)\n",
    "    metadata_list.append({\n",
    "        'filename': filename,\n",
    "        'sentinel_meta': sentinel_meta,\n",
    "        'treecover_meta': treecover_meta\n",
    "    })\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "X_tensor = torch.tensor(np.stack(X_images, axis=0))\n",
    "y_tensor = torch.tensor(np.stack(y_images, axis=0), dtype=torch.float32)\n",
    "\n",
    "# Serialize tensors and save to disk\n",
    "torch.save(X_tensor, X_test_tensor_name)\n",
    "torch.save(y_tensor, y_test_tensor_name)\n",
    "\n",
    "# Save metadata to JSON file\n",
    "save_metadata(metadata_list, 'metadata_test.json')\n",
    "\n",
    "print(\"Data preparation and serialization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data was succesfully loaded into the tensors `'X_test_tensor.pth'` and `'y_test_tensor.pth'`\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "## **LOADING OUR SERIALIZED MODELS AND THE DATASET**\n",
    "\n",
    "<br>\n",
    "\n",
    "To load our models, first we must make sure that all the required libraries, function and dependencies are available in our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor_name = 'X_test_tensor_visuals.pth'\n",
    "y_test_tensor_name = 'y_test_tensor_visuals.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# custom definitions\n",
    "from src.classes.VGGUdeaSpectral import VGGUdeaSpectral\n",
    "\n",
    "from src.classes.MultipleRegressionModel import MultipleRegressionModel\n",
    "\n",
    "\n",
    "def rmse_score(net, X, y):\n",
    "    y_pred = net.predict(X)\n",
    "    rmse = (mean_squared_error(y_true=y, y_pred=y_pred)) ** 0.5\n",
    "    return -rmse  # Skorch tries to maximize the score, so negate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_tensor: torch.Size([64106, 3, 100, 100])\n",
      "Data type of X_tensor: torch.uint8\n",
      "Number of elements in X_tensor: 1923180000\n",
      "Shape of y_tensor: torch.Size([64106, 1])\n",
      "Data type of y_tensor: torch.float32\n",
      "Number of elements in y_tensor: 64106\n",
      "Size of X_tensor in bytes: 1923180000\n",
      "Size of y_tensor in bytes: 256424\n"
     ]
    }
   ],
   "source": [
    "# Load the tensors\n",
    "X_test_tensor = torch.load(X_test_tensor_name)\n",
    "y_test_tensor = torch.load(y_test_tensor_name).view(-1,1)\n",
    "\n",
    "# Print the shapes and data types of the tensors\n",
    "print(\"Shape of X_tensor:\", X_test_tensor.shape)\n",
    "print(\"Data type of X_tensor:\", X_test_tensor.dtype)\n",
    "print(\"Number of elements in X_tensor:\", torch.numel(X_test_tensor))\n",
    "\n",
    "print(\"Shape of y_tensor:\", y_test_tensor.shape)\n",
    "print(\"Data type of y_tensor:\", y_test_tensor.dtype)\n",
    "print(\"Number of elements in y_tensor:\", torch.numel(y_test_tensor))\n",
    "\n",
    "# Calculate the size in bytes\n",
    "print(\"Size of X_tensor in bytes:\", X_test_tensor.element_size() * X_test_tensor.nelement())\n",
    "print(\"Size of y_tensor in bytes:\", y_test_tensor.element_size() * y_test_tensor.nelement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;src.classes.VGGUdeaSpectral.VGGUdeaSpectral&#x27;&gt;,\n",
       "  module__num_bands=3,\n",
       "),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={&#x27;lr&#x27;: [0.01, 0.001, 0.0001],\n",
       "                                        &#x27;max_epochs&#x27;: [5, 10, 20],\n",
       "                                        &#x27;module__activation_type&#x27;: [&#x27;relu&#x27;,\n",
       "                                                                    &#x27;sigmoid&#x27;,\n",
       "                                                                    &#x27;tanh&#x27;],\n",
       "                                        &#x27;module__dropout_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x71b1a85364a0&gt;,\n",
       "                                        &#x27;module__fc1_out_features&#x27;: [512, 1024,\n",
       "                                                                     2048],\n",
       "                                        &#x27;module__fc2_out_features&#x27;: [256, 512,\n",
       "                                                                     1024],\n",
       "                                        &#x27;module__num_filters1&#x27;: [32, 64, 128],\n",
       "                                        &#x27;module__num_filters2&#x27;: [64, 128, 256],\n",
       "                                        &#x27;module__num_filters3&#x27;: [128, 256,\n",
       "                                                                 512]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;src.classes.VGGUdeaSpectral.VGGUdeaSpectral&#x27;&gt;,\n",
       "  module__num_bands=3,\n",
       "),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={&#x27;lr&#x27;: [0.01, 0.001, 0.0001],\n",
       "                                        &#x27;max_epochs&#x27;: [5, 10, 20],\n",
       "                                        &#x27;module__activation_type&#x27;: [&#x27;relu&#x27;,\n",
       "                                                                    &#x27;sigmoid&#x27;,\n",
       "                                                                    &#x27;tanh&#x27;],\n",
       "                                        &#x27;module__dropout_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x71b1a85364a0&gt;,\n",
       "                                        &#x27;module__fc1_out_features&#x27;: [512, 1024,\n",
       "                                                                     2048],\n",
       "                                        &#x27;module__fc2_out_features&#x27;: [256, 512,\n",
       "                                                                     1024],\n",
       "                                        &#x27;module__num_filters1&#x27;: [32, 64, 128],\n",
       "                                        &#x27;module__num_filters2&#x27;: [64, 128, 256],\n",
       "                                        &#x27;module__num_filters3&#x27;: [128, 256,\n",
       "                                                                 512]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;src.classes.VGGUdeaSpectral.VGGUdeaSpectral&#x27;&gt;,\n",
       "  module__num_bands=3,\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;src.classes.VGGUdeaSpectral.VGGUdeaSpectral&#x27;&gt;,\n",
       "  module__num_bands=3,\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
       "  module=<class 'src.classes.VGGUdeaSpectral.VGGUdeaSpectral'>,\n",
       "  module__num_bands=3,\n",
       "),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'lr': [0.01, 0.001, 0.0001],\n",
       "                                        'max_epochs': [5, 10, 20],\n",
       "                                        'module__activation_type': ['relu',\n",
       "                                                                    'sigmoid',\n",
       "                                                                    'tanh'],\n",
       "                                        'module__dropout_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x71b1a85364a0>,\n",
       "                                        'module__fc1_out_features': [512, 1024,\n",
       "                                                                     2048],\n",
       "                                        'module__fc2_out_features': [256, 512,\n",
       "                                                                     1024],\n",
       "                                        'module__num_filters1': [32, 64, 128],\n",
       "                                        'module__num_filters2': [64, 128, 256],\n",
       "                                        'module__num_filters3': [128, 256,\n",
       "                                                                 512]},\n",
       "                   scoring='neg_mean_absolute_error', verbose=3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LOADING MODEL VGGUdeaSpectral\n",
    "import joblib\n",
    "from src.classes.VGGUdeaSpectral import VGGUdeaSpectral\n",
    "\n",
    "\n",
    "vgg_model = joblib.load(\"src/trained_models/VGGUdeaSpectral1/VGGUdeaSpectral_model1.joblib\")\n",
    "\n",
    "vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING MODEL VGGUdeaSpectral\n",
    "\n",
    "from src.classes.VGGUdeaSpectral import VGGUdeaSpectral\n",
    "\n",
    "\n",
    "vgg_model = joblib.load(\"src/trained_models/VGGUdeaSpectral0/VGGUdeaSpectral_model0.joblib\")\n",
    "\n",
    "vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=MultipleRegressionModel(), n_jobs=2,\n",
       "             param_grid={&#x27;copy_X&#x27;: [True, False],\n",
       "                         &#x27;fit_intercept&#x27;: [True, False],\n",
       "                         &#x27;positive&#x27;: [True, False]},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=MultipleRegressionModel(), n_jobs=2,\n",
       "             param_grid={&#x27;copy_X&#x27;: [True, False],\n",
       "                         &#x27;fit_intercept&#x27;: [True, False],\n",
       "                         &#x27;positive&#x27;: [True, False]},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MultipleRegressionModel</label><div class=\"sk-toggleable__content\"><pre>MultipleRegressionModel()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultipleRegressionModel</label><div class=\"sk-toggleable__content\"><pre>MultipleRegressionModel()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MultipleRegressionModel(), n_jobs=2,\n",
       "             param_grid={'copy_X': [True, False],\n",
       "                         'fit_intercept': [True, False],\n",
       "                         'positive': [True, False]},\n",
       "             scoring='neg_mean_absolute_error', verbose=3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LOADING MODEL MultipleRegression\n",
    "\n",
    "from src.classes.VGGUdeaSpectral import VGGUdeaSpectral\n",
    "\n",
    "\n",
    "multiple_regression_model = joblib.load(\"src/trained_models/MultipleRegression0/MultipleRegression_model0.joblib\")\n",
    "\n",
    "multiple_regression_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 5.838313579559326\n",
      "Average RMSE: 7.768616676330566\n"
     ]
    }
   ],
   "source": [
    "# Convert X to float32\n",
    "X_test_tensor = X_test_tensor.to(dtype=torch.float32)\n",
    "\n",
    "predictions = vgg_model.predict(X_test_tensor)\n",
    "\n",
    "\n",
    "# If  y is a PyTorch tensor, you need to convert it to a NumPy array\n",
    "y_np = y_test_tensor.cpu().numpy() if isinstance(y_test_tensor, torch.Tensor) else y_test_tensor\n",
    "\n",
    "# Calculate MAE and RMSE using the true labels y\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y_np, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_np, predictions))\n",
    "\n",
    "print(f'Average MAE: {mae}')\n",
    "print(f'Average RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Info: \n",
      "DAtatype:     <class 'numpy.ndarray'>\n",
      "Shape:        (64106, 1)\n",
      "Unique Values as percentage of Vegetativa Coverage per 1km^2 tile: \n",
      "\n",
      "[46.53532  46.53537  46.535534 ... 94.0871   94.39075  94.58438 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions Info: \")\n",
    "print(\"DAtatype:    \", type(predictions))\n",
    "print(\"Shape:       \", predictions.shape)\n",
    "\n",
    "print(\"Unique Values as percentage of Vegetativa Coverage per 1km^2 tile: \\n\")\n",
    "\n",
    "print(np.unique(predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 4.6125633849907945\n",
      "Average RMSE: 9.391506176502144\n"
     ]
    }
   ],
   "source": [
    "# Convert X to float32\n",
    "X_test_tensor = X_test_tensor.to(dtype=torch.float32)\n",
    "\n",
    "# Flatten the images\n",
    "X_test = X_test_tensor.reshape(X_test_tensor.shape[0], -1)\n",
    "\n",
    "predictions = multiple_regression_model.predict(X_test)\n",
    "\n",
    "# If  y is a PyTorch tensor, you need to convert it to a NumPy array\n",
    "y_np = y_test_tensor.cpu().numpy() if isinstance(y_test_tensor, torch.Tensor) else y_test_tensor\n",
    "\n",
    "# Calculate MAE and RMSE using the true labels y\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y_np, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_np, predictions))\n",
    "\n",
    "print(f'Average MAE: {mae}')\n",
    "print(f'Average RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## **CONCLUSIONS**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **FOR THE VGGUdeaSpectral Model**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "```javascript\n",
    "Validation Set (2019):\n",
    "\n",
    "MAE: 3.9198\n",
    "RMSE: 6.394\n",
    "\n",
    "\n",
    "Test Set (2020):\n",
    "\n",
    "MAE: 5.8383\n",
    "RMSE: 7.7686\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "We observe that the model's performance deteriorated by approximately 48.94% in Mean Absolute Error (MAE) and 21.50% in Root Mean Squared Error (RMSE) when tested on the 2020 images compared to the validation performance on the 2019 data. This substantial deterioration might suggest:\n",
    "\n",
    "Temporal Overfitting: The model may be significantly overfitting to the specific conditions of 2019, including weather patterns, seasonal vegetation changes, or specific events (like fires or deforestation) that uniquely characterized that year.\n",
    "\n",
    "Shift in Data Distribution: The characteristics of the images from 2020 could differ markedly from those of 2019, indicating a significant distribution shift. This discrepancy could be due to various factors, such as changes in satellite imaging conditions, alterations in land use, or natural vegetation cycles.\n",
    "\n",
    "Model Robustness: The pronounced performance drop highlights potential issues with the model's robustness, suggesting it may struggle with the year-to-year natural variability in the data. This indicates a need for incorporating a more diverse dataset spanning multiple years during training to enhance temporal generalization.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **FOR THE MultipleRegressor Model**\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "```javascript\n",
    "Validation Set (2020):\n",
    "\n",
    "MAE: 4.659\n",
    "RMSE: 9.1532\n",
    "\n",
    "\n",
    "Test Set (2019):\n",
    "\n",
    "MAE: 5.8381\n",
    "RMSE: 9.3915\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "The performance of the MultipleRegressor model on the 2019 dataset compared to its validation on the 2020 dataset shows:\n",
    "\n",
    "- A slight improvement in Mean Absolute Error (MAE) by approximately 1.00%.\n",
    "- A deterioration in Root Mean Squared Error (RMSE) by approximately 2.60%.\n",
    "\n",
    "This suggest that:\n",
    "\n",
    "1. The MAE improvement suggests that on average, the model's predictions were marginally closer to the actual values in 2019 compared to 2020, despite being trained on 2020 data.\n",
    "\n",
    "2. The increase in RMSE indicates that there were likely a few larger errors in the 2019 predictions, as RMSE is more sensitive to larger errors due to the squaring of errors.\n",
    "\n",
    "3. Overall, the model shows a relatively stable performance across the two years, with a small deterioration in RMSE, which is to be expected due to variations in data year-to-year.​"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
